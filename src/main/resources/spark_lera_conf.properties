# Spark Configs

#(spark.app.name) => Spark application name
spark.app.name=etl_loader

# log_level options INFO, DEBUG, OFF, WARN, FATAL
spark.log_level=INFO

# Environment info

# Configuration DB names
spark.audit_database=etl_audit
spark.config_database=etl_config

# Configuration tables
spark.audit=audit
spark.config=generic_config
spark.default_values_table=default_values
spark.column_mapping_table=column_mapping
spark.join_table=join_table_mapping
spark.delete_table=delete_data
spark.filter_table=filter_data
spark.partition_table=partition_column
spark.generic_lookup_table=generic_lookup

# JDBC Connection Info
#The below properties should be used when using impala driver to connect
#Currently in this project, we are using hive jdbc driver to connect

#spark.impala_username=user@AP.CORP.LERA.COM
#spark.impala_password=Cars@123!
#spark.impalaConnectionURL=jdbc:impala://cluster-impala.org.com:21050
#spark.impalaJDBCDriver=com.cloudera.impala.jdbc41.driver

#spark.hive_username=administrator
#spark.hive_password=Welcome12345

spark.hive_username=spark
spark.hive_password=welcome

spark.hiveConnectionURL=jdbc:hive2://10.22.1.66:2181,10.22.1.66:2181:2181,10.22.1.67:2181/default;password=spark;serviceDiscoveryMode=zooKeeper;user=spark;zooKeeperNamespace=hiveserver2
spark.hiveJDBCDriver=org.apache.hive.jdbc.HiveDriver

# Applicable for column mapping
spark.skip_missing_source_column_source_systems=
# Applicable for column mapping and default values
spark.skip_missing_target_column_source_systems=
spark.excel_sheet_name=Sheet1
spark.kudu_default_partitions=200
spark.read_kudu_using_hive_source_systems=

#Set the source system transformer to hive as below and this will use hive writer to write the data
#spark.agrosoft_inventory_writers=hive

spark.smtp_host=myorg.com
spark.notification_mail_list=issues@myorg.com
spark.notification_mail_subject=Issue with ETL

#Run time should be specified in minutes
spark.default_max_run_time=120
spark.default_notification_interval_time=30
spark.job_exec_control_table=job_execution_control
spark.generic_config_transformers=TypeCast

# Transformation based on source system
#spark.condition_table=cascna_jde_lookup_mapping

#spark.rtduet_transformers=ColumnMap,DefaultValue,RtDuet,Joiner,TypeCast
#spark.jupiter_oc_transformers=ColumnMap,DefaultValue,Jupiter_oc,Joiner,TypeCast
##spark.jupiter_ld_transformers=ColumnMap,DefaultValue,Jupiter_ld,Joiner,TypeCast
#spark.magellan_transformers=ColumnMap,DefaultValue,Magellan,Joiner,Delete,TypeCast
#spark.open_contracts_transformers=Delete,Joiner,Lynx_open_contracts,ColumnMap,DefaultValue
##spark.agrosoft_transformers=Delete,ColumnMap,DefaultValue,Joiner,Agrosoft,TypeCast
#spark.lynx_inventory_transformers=Joiner,Lynx_Inventory,ColumnMap,DefaultValue,TypeCast
#spark.agrosoft_inventory_transformers=ColumnMap,DefaultValue,Agrosoft_Inventory,TypeCast
#spark.lynx_commodity_transformers=ColumnMap,DefaultValue,TypeCast
#spark.jde_volume_transformers=JDEVolume,ColumnMap,DefaultValue,TypeCast
#spark.lynx_commodity_movement_transformers=ColumnMap,DefaultValue,Lynx_Commodity_Movement
#spark.lynx_facilities_transformers=ColumnMap,DefaultValue,TypeCast
#spark.lynx_cash_position_stg_transformers=ColumnMap,DefaultValue,Filter,TypeCast
#spark.lynx_cash_position_transformers=ColumnMap,DefaultValue,Lynx_Cash_Position,TypeCast
#spark.towworks_cp_stg1_transformers=TowWorksstg,TypeCast
#spark.towworks_cp_stg2_transformers=TowWorksstg,TypeCast
#spark.towworks_cp_transformers=Filter,TowWorks,ColumnMap,DefaultValue,TypeCast
#spark.macbeth_transformers=ColumnMap,DefaultValue,MacBeth,TypeCast
#spark.towworks_barges_stg1_transformers=TowWorksstg,TypeCast
#spark.towworks_barges_stg2_transformers=TowWorksstg,TypeCast
#spark.towworks_barges_transformers=DefaultValue,ColumnMap,TowWorksBarges,TypeCast
#spark.towworks_contracts_position_transformers=TowWorksContracts,ColumnMap,DefaultValue
#spark.agrosoft_commodity_transformers=ColumnMap,DefaultValue,TypeCast
#spark.jde_commodity_movements_transformers=ColumnMap,TypeCast
#spark.lynx_org_organization_transformers=ColumnMap,DefaultValue,TypeCast
#spark.agrosoft_org_organization_transformers=ColumnMap,DefaultValue,TypeCast
#spark.voyages_stg1_transformers=ColumnMap,DefaultValue,BSMVoyagesstg,TypeCast

# Transformation based on filter or delete condition from Source to Staging Ingestion
#spark.lynx_inventory_stg_transformers=ColumnMap,DefaultValue,Filter,TypeCast
#spark.agrosoft_inventory_stg_transformers=ColumnMap,DefaultValue,Delete,TypeCast
#spark.jde_finance_transformers=JDE,ColumnMap,DefaultValue,TypeCast
#spark.facility_transformers=ColumnMap,DefaultValue,Facility,TypeCast
#spark.agrosoft_contracts_transformers=Joiner,LOOKUP,Agrosoft_contracts,ColumnMap,DefaultValue
#spark.trip_status_stg2_transformers=Trip_status_stg2
#spark.trip_status_transformers=Joiner,Trip_status,ColumnMap,DefaultValue,TypeCast
#spark.lynx_oc_transformers=Delete,Joiner,Lynx_oc,ColumnMap,DefaultValue,TypeCast
#spark.magellan_exp_projections_transformers=Joiner,ColumnMap,DefaultValue,magellan_exp_projection
#spark.JDE_Inventory_transformers=TypeCast
#spark.JDE_Inventory2_transformers=JDE_Inventory2,TypeCast
#spark.lynx_inventory_writers=hive
#spark.agrosoft_inventory_writers=hive
#spark.truncate_target_based_source_systems=trip_status,agrosoft,open_contracts,agrosoft

#spark.jde_contracts_stg_transformers=Filter
#spark.jde_contracts_transformers=Joiner,Jde_contracts,ColumnMap,DefaultValue,TypeCast

